{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmq-7x_y81wX"
   },
   "source": [
    "# Word2Vec Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQctzlnJ81wa"
   },
   "source": [
    "First, download the file from [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) and place it in the same directory on your machine as this notebook.\n",
    "\n",
    "(If you're on colab, you'll need to upload this file to your colab project.)\n",
    "\n",
    "Note that this will download a giant 1.5GB file onto your computer!\n",
    "\n",
    "**DO NOT unzip/decompress/double-click on this file once it's downloaded!** (on some computers, this means that the original compressed file is deleted, but we need it in that format for this notebook to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-7oJpIl81wb"
   },
   "outputs": [],
   "source": [
    "#Run this every time you run the notebook \n",
    "embeddings_file = \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to install gensim by opening a terminal/command window and typing:\n",
    "\n",
    "`conda install gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Odcg6Zo81wc"
   },
   "source": [
    "## Colab and own-computer users, everyone now continue here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeAmdvB381wc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYfYx5my81wc"
   },
   "outputs": [],
   "source": [
    "#This line will load the 200,000 most common words into wv, a variable of word vectors\n",
    "#Note that this will load these into memory, so if you don't have a lot of memory on your computer you may run into problems/slowness\n",
    "wv = KeyedVectors.load_word2vec_format(embeddings_file, binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEY1sn4s81wd"
   },
   "outputs": [],
   "source": [
    "#View the actual value of a vector for a word:\n",
    "# this is a 300-dimensional vector\n",
    "print(\"Shape of a word vector: \")\n",
    "print(np.shape(wv['language'])) #play around with other words\n",
    "print(\"Word vector for 'language':\")\n",
    "print(wv['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAPGYXkr81wd"
   },
   "outputs": [],
   "source": [
    "#We can compare words by computing their cosine similarity\n",
    "# Try this out for different word pairs:\n",
    "print(wv.similarity('dog', 'cat'))\n",
    "print(wv.similarity('dog', 'terrier'))\n",
    "print(wv.similarity('dog', 'dolphin'))\n",
    "print(wv.similarity('dog', 'aubergine'))\n",
    "print(wv.similarity('dog', 'exuberant'))\n",
    "print(wv.similarity('dog', 'economics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiY0EO0781wd"
   },
   "outputs": [],
   "source": [
    "# Look for most similar words to a given word\n",
    "#Try this out for different words\n",
    "wv.most_similar(positive=['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqzhpqYD81we"
   },
   "outputs": [],
   "source": [
    "# Look for most similar words to a set of words\n",
    "#wv.most_similar(positive=['potato', 'yam', 'yucca'])\n",
    "#wv.most_similar(positive=['croissant', 'donut'])\n",
    "wv.most_similar(positive=['London', 'Paris']) #Or maybe Camberwell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dasAHs1e81we",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Try it with people\n",
    "wv.most_similar(positive=['Boris_Johnson','Gordon_Brown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1bOgIIR81we"
   },
   "outputs": [],
   "source": [
    "# Let's try answering a question via adding vectors: What are some snacks that people like in England?\n",
    "wv.most_similar(positive=['England','snack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwwWjY1L81wf"
   },
   "outputs": [],
   "source": [
    "# Now let's experiment with subtracting vectors...\n",
    "# the \"positive\" words point in the direction(s) you want to go; the \"negative\" words' vectors are subtracted from these\n",
    "\n",
    "#Example from powerpoint: goose + (dogs - dog)\n",
    "wv.most_similar(positive=['goose','dogs'],negative=['dog']) #It's geese!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4H6D9_-81wf"
   },
   "outputs": [],
   "source": [
    "#Another way of phrasing the above: Dog is to dogs as goose is to what?\n",
    "\n",
    "#Let's try some more\n",
    "# parts of speech: \"Longest is to long as slowest is to what?\"\n",
    "wv.most_similar(positive=['slowest','long'],negative=['longest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XKxHYG981wf"
   },
   "outputs": [],
   "source": [
    "# gender:\"man is to king as as woman is to what?\"\n",
    "wv.most_similar(positive=['woman','king'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOjNvCsq81wg"
   },
   "outputs": [],
   "source": [
    "#'man' minus 'woman' gives us a 'manly' vector. What happens when we subtract this vector from 'doctor'? \n",
    "wv.most_similar(positive=['woman','doctor'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrwLNlPL81wg"
   },
   "outputs": [],
   "source": [
    "#'man' minus 'woman' gives us a 'manly' vector. What happens when we ADD this vector to 'doctor'? \n",
    "# Can do this explicitly in vector math and get nearly the same result as most_similar function\n",
    "X = wv['man'] - wv['woman']\n",
    "v = (wv['doctor'] + X)\n",
    "wv.similar_by_vector(v) #grabs the most similar words to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q76fVVdC81wg"
   },
   "outputs": [],
   "source": [
    "#Places: \"UK is to London as as France is to what?\"\n",
    "wv.most_similar(positive=['France','London'],negative=['UK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhuF7cTo81wg"
   },
   "outputs": [],
   "source": [
    "#What is the bagel of London?\n",
    "wv.most_similar(positive=['London','bagel'],negative=['New_York'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_Z7ewvR81wg"
   },
   "outputs": [],
   "source": [
    "#Who is the Marie Curie of music?\n",
    "wv.most_similar(positive=['Marie_Curie', 'music'],negative=['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbMKVh7I81wg"
   },
   "outputs": [],
   "source": [
    "#Finally, can search for things that don't match:\n",
    "wv.doesnt_match(\"dog cat lion rhino platypus\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0-LQ9dp81wh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#More exploration of bias\n",
    "print(wv.similarity('man', 'nurse'))\n",
    "print(wv.similarity('woman','nurse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqhkH6SL81wh"
   },
   "outputs": [],
   "source": [
    "print(wv.similarity('man', 'intelligent'))\n",
    "print(wv.similarity('woman', 'intelligent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGzZPqzr81wh"
   },
   "outputs": [],
   "source": [
    "print(wv.similarity('man', 'competent'))\n",
    "print(wv.similarity('woman', 'competent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sER97Dq81wh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wv.similarity('man', 'attractive'))\n",
    "print(wv.similarity('woman', 'attractive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsngUU2L81wh"
   },
   "outputs": [],
   "source": [
    "#Keep playing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKgWOEun81wh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Week 4.2-Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
