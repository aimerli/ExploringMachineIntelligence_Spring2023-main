{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Build an LSTM to build a classifier for the Human Activity Recognition Using Smartphones Dataset #\n",
    "\n",
    "Adapted from https://github.com/servomac/Human-Activity-Recognition/\n",
    "\n",
    "Dataset information at https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have not yet installed wget, you will need to start by doing so:**\n",
    "\n",
    "1) Open a new terminal/command window\n",
    "2) Type `conda install python-wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All our imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from matplotlib.pyplot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell uses \"wget\" to download data from an online archive. You'll only need to run this the first time you run the notebook, because when you run this the first time it will save the data in the directory with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you already have it!\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "#This is going to take a little while\n",
    "if not os.path.isdir(\"UCI HAR Dataset\"):\n",
    "    print(\"downloading\")\n",
    "    wget.download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\")\n",
    "    \n",
    "    with zipfile.ZipFile(\"UCI HAR Dataset.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"\")\n",
    "        \n",
    "    print(\"done\")\n",
    "    \n",
    "else:\n",
    "    print(\"you already have it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is helper code from the github repo.\n",
    "#No need to edit this, but it's helpful to glance through this code to see what it does\n",
    "\n",
    "DATADIR = 'UCI HAR Dataset' \n",
    "\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]\n",
    "\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'{DATADIR}/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).values\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'{DATADIR}/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).values\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our LSTM model ##\n",
    "\n",
    "This is where the important stuff happens. Try to understand everything happening here, and experiment with changing it once you've got the basic method working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and splits into training and test sets, where X is input and Y is output\n",
    "X_train, X_test, Y_train, Y_test = load_data()\n",
    "\n",
    "#Compute some information from the data, which we need to set up the model\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After loading data, it's always a good idea to get a feel for what is in the data \n",
    "\n",
    "#For instance, we can use np.shape() to look at the shape of our examples\n",
    "print(np.shape(X_train)) #this shows us we have 7352 training examples, where each input is 128x9 (9 channels of 128 samples each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can peek at an individual example\n",
    "exampleNumber = 0\n",
    "channelNumber = 3\n",
    "plot(np.transpose(X_train[exampleNumber])[channelNumber])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Y_train)) #and we have a 6-dimensional output for each one of our 7352 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're using a \"one-hot\" encoding, which means that \"1 0 0 0 0 0\" means \"class 1\", \"0 1 0 0 0 0\" means class 2, and so on\n",
    "print(Y_train[0]) #Prints the 6-dimensional output for the first training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is where things get fun.\n",
    "\n",
    "# Set some parameters to control training\n",
    "# The number of epochs is the number of passes through the training data to make before you stop training\n",
    "epochs = 5 # Play with this; might especially want to increase it once you know things are basically working\n",
    "\n",
    "# The batch size is the number of examples to use to compute the gradient before updating the weights\n",
    "# If batch_size is the same as the training set size, we use the whole training set every time. This is called \"batch gradient descent\"\n",
    "# If batch_size is 1 then this is \"stochastic gradient descent\"\n",
    "# If batch_size is something in between, then this is called \"mini-batch\" gradient descent\n",
    "# Each choice will have different tradeoffs in terms of training time and ultimate accuracy, but there's no \"right way\" to do it\n",
    "# For more info, see https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/#:~:text=The%20batch%20size%20is%20a%20hyperparameter%20that%20defines%20the%20number,updating%20the%20internal%20model%20parameters.&text=When%20the%20batch%20size%20is,called%20mini%2Dbatch%20gradient%20descent. \n",
    "batch_size = 16 #You might want to play with this too\n",
    "\n",
    "#How many hidden neurons do we want?\n",
    "n_hidden = 32 #Feel free to change this\n",
    "\n",
    "\n",
    "#This is the Keras code for specifying the network architecture:\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim))) #an LSTM layer\n",
    "model.add(Dropout(0.5)) #This is a method to try to prevent overfitting. See more at https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "model.add(Dense(n_classes, activation='sigmoid')) #a plain \"dense\" layer comes after the LSTM layer\n",
    "\n",
    "#This line specifies how to train this network.\n",
    "#You might want to play with these 3 values\n",
    "#see more at https://keras.io/api/losses/, https://keras.io/api/optimizers/, https://keras.io/api/metrics/\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#This line of code actually trains the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next steps: Experiment!\n",
    "\n",
    "### 1. Try digging into & tweaking the results above. ###\n",
    "\n",
    "* What happens with a different optimizer?\n",
    "* Can you edit the code to use a validation set with early stopping, as well?\n",
    "* What happens to the training accuracy, test/validation accuracy, and training time if you change the number of training examples?\n",
    "* What happens if you change the dropout?\n",
    "* Can you visualise this data (e.g., in Python using matplotlib, or in Excel) to get a sense of what it looks like?\n",
    "* What else might you print each epoch, instead of just accuracy and loss?\n",
    "* What else might you do if you were trying to release a commercial product that did this classification? How would you test it? How else might you edit the code to train your neural network? What would your main considerations be in deploying it?\n",
    "\n",
    "### 2. Do more reading about all the options available to you in Keras ###\n",
    "* https://keras.io/api/optimizers/\n",
    "* https://keras.io/api/metrics/\n",
    "* https://keras.io/api/losses/\n",
    "* Use the `?` operation in Python to learn more about your options in code (e.g., `?model.compile`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
